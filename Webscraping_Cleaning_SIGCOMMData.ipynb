{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vrathi101/SIGCOMMInclusivity/blob/main/Webscraping_Cleaning_SIGCOMMData.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install and import necessary Python libraries for subsequent code (these libraries will be used throughout the Google Colabs.\n",
        "1. Fuzzywuzzy is used for string matching (comparing strings which aren't identical but may have differences caused by typos, abbreviations, etc.). I used this when there weren't exact matches between the committee member dataframe and the researchers csv.\n",
        "2. Levenshtein is used to measure the \"distance\" between strings which will be used for fuzzy comparing.\n",
        "3. Beautifulsoup4 is used for webscraping. I used webscraping to extract the committee members for each year for SIGCOMM.\n",
        "4. Gender-guesser is used to guess the gender of a person based on their first name. It have 6 possible outputs: 'male', 'mostly_male', 'female', 'mostly_female', 'andy', and 'unknown'. I used this for the names which had a fuzzy ratio of less than the fixed threshold when compared to every name in the researchers csv.\n",
        "5. Unidecode is used to turn weird characters, like letters with accents or unprintable characters, into regular ones to make it easier for matching.\n",
        "6. GeonamesCache was used to convert country names into the continents where they are located.\n",
        "7. GeoText is used to extract words like a city or location from a longer text.\n",
        "8. Ipywidgets adds interactivity to dataframes where users can visualize and organize the data as they wish. The library uses click events to show the desired results."
      ],
      "metadata": {
        "id": "CFiK2XMHZML5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6sk4SH1oY595",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a2fb205-b17f-44e7-dfa8-4228358b5d64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fuzzywuzzy\n",
            "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
            "Installing collected packages: fuzzywuzzy\n",
            "Successfully installed fuzzywuzzy-0.18.0\n",
            "Collecting levenshtein\n",
            "  Downloading Levenshtein-0.21.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (172 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.5/172.5 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rapidfuzz<4.0.0,>=2.3.0 (from levenshtein)\n",
            "  Downloading rapidfuzz-3.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, levenshtein\n",
            "Successfully installed levenshtein-0.21.1 rapidfuzz-3.1.2\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.27.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.11.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.4.1)\n",
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.9/235.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.6\n",
            "Collecting GeonamesCache\n",
            "  Downloading geonamescache-2.0.0-py3-none-any.whl (26.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: GeonamesCache\n",
            "Successfully installed GeonamesCache-2.0.0\n",
            "Collecting GeoText\n",
            "  Downloading geotext-0.4.0-py2.py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: GeoText\n",
            "Successfully installed GeoText-0.4.0\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (7.7.1)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (5.5.6)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (3.6.4)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (7.34.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (3.0.8)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.3.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (67.7.2)\n",
            "Collecting jedi>=0.16 (from ipython>=4.0.0->ipywidgets)\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (3.0.39)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (2.14.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (4.8.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets) (6.4.8)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.8.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.1.2)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (23.2.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (21.3.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (5.3.1)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (5.9.1)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (6.5.4)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.5.6)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.2)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.17.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.17.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.2.6)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.6.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.9.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.1->jupyter-client->ipykernel>=4.5.1->ipywidgets) (1.16.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (21.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.1.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.9.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.11.2)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (6.0.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.4)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.2.2)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.8.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (23.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.5.0)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.2.1)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.17.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.3.3)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (23.1.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.19.3)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.15.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.4.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.21)\n",
            "Installing collected packages: jedi\n",
            "Successfully installed jedi-0.18.2\n"
          ]
        }
      ],
      "source": [
        "#install all packages\n",
        "!pip install fuzzywuzzy\n",
        "!pip install levenshtein\n",
        "!pip install requests beautifulsoup4\n",
        "!pip install unidecode\n",
        "!pip install GeonamesCache\n",
        "!pip install GeoText\n",
        "!pip install ipywidgets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import all of the libraries we will be using."
      ],
      "metadata": {
        "id": "nVLh97NEZqTe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from fuzzywuzzy import fuzz\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import matplotlib.pyplot as plt\n",
        "from unidecode import unidecode\n",
        "import re\n",
        "from geonamescache import GeonamesCache\n",
        "from geotext import GeoText"
      ],
      "metadata": {
        "id": "TMX-KzNUZDBj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jU_u_oEt3u8",
        "outputId": "8ead2a8c-0efb-4ca8-bd99-8c4c7253d4b8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using webscraping to extract all the organizational committee and program committee members and their institutions for SIGCOMM from 2009-2023, inclusive, and then adding all the names/regions, categorized by year and committee type, into a dataframe."
      ],
      "metadata": {
        "id": "OEdS-KNuZwbl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an empty DataFrame with columns for name, year, and committee\n",
        "dfSigcomm = pd.DataFrame(columns=['name', 'year', 'committee','region'])\n",
        "committee=\"Organizing\"\n",
        "# Loop through the years 2009-2023\n",
        "for year in range(2009, 2024):\n",
        "  # Determine the URL based on the year\n",
        "  if year >= 2012:\n",
        "    if year in [2016, 2014, 2013, 2012]:\n",
        "      url = f\"https://conferences.sigcomm.org/sigcomm/{year}/organization.php\"\n",
        "    elif year == 2015:\n",
        "      url = \"https://conferences.sigcomm.org/sigcomm/2015/organisation.php\"\n",
        "    else:\n",
        "      url = f\"https://conferences.sigcomm.org/sigcomm/{year}/org-committee.html\"\n",
        "    # Send a GET request to the URL and retrieve the response\n",
        "    response = requests.get(url)\n",
        "    # Create a BeautifulSoup object to parse the HTML response\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "    # Select the HTML elements that contain the names of committee members\n",
        "    name_elements = soup.select('div.ui-grid-a div.ui-block-a p')\n",
        "    # Extract the names from the HTML elements and strip any whitespace\n",
        "    member_name = [name.text.strip() for name in name_elements]\n",
        "    # Select the HTML elements that contain the regions of committee members\n",
        "    region_elements = soup.select('div.ui-grid-a div.ui-block-b p')\n",
        "    # Extract the regions from the HTML elements and strip any whitespace\n",
        "    region_name = [region.text.strip() for region in region_elements]\n",
        "    #add data to a dataframe and concatenate with the original\n",
        "    df = pd.DataFrame({'name': member_name, 'year': year, 'committee': committee, 'region': region_name})\n",
        "    dfSigcomm = pd.concat([dfSigcomm, df], ignore_index=False)\n",
        "  elif year in [2011, 2010, 2009]:\n",
        "      # Create the URL with the specified year\n",
        "      url = f\"https://conferences.sigcomm.org/sigcomm/{year}/organization.php\"\n",
        "      # Send a GET request to the URL and retrieve the response\n",
        "      response = requests.get(url)\n",
        "      # Create a BeautifulSoup object to parse the HTML response\n",
        "      soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "      # Find the <div> element with the id \"contents\"\n",
        "      contents_div = soup.find(\"div\", id=\"contents\")\n",
        "      if contents_div:\n",
        "          if year == 2010:\n",
        "              # Find the <h2> element with the text \"Organizing Committee\"\n",
        "              committee_header = contents_div.find(\"h2\", string=\"Organizing Committee\")\n",
        "          else:\n",
        "              # Find the <h2> element with the text \"Committee\"\n",
        "              committee_header = contents_div.find(\"h2\", string=\"Committee\")\n",
        "          if committee_header:\n",
        "              # Find the table following the <h2> element\n",
        "              committee_table = committee_header.find_next_sibling(\"table\")\n",
        "              if committee_table:\n",
        "                  # Find all <tr> elements within the table's <tbody>\n",
        "                  committee_members = committee_table.find_all(\"tr\")\n",
        "                  for member in committee_members:\n",
        "                      # Find the first <td> element within each <tr>\n",
        "                      name_cell = member.find_all(\"td\")\n",
        "                      if name_cell:\n",
        "                          member_name = name_cell[0].get_text(strip=True)\n",
        "                          if len(name_cell) >= 2:\n",
        "                              region = name_cell[1]\n",
        "                              region_name = region.get_text(strip=True)\n",
        "                              # Add the name to the final DataFrame\n",
        "                              df = pd.DataFrame({'name': [member_name], 'year': year, 'committee': 'Organizing', 'region': [region_name]})\n",
        "                              dfSigcomm = pd.concat([dfSigcomm, df], ignore_index=True)\n",
        "                          else:\n",
        "                              df = pd.DataFrame({'name': [member_name], 'year': year, 'committee': 'Organizing'})\n",
        "                              dfSigcomm = pd.concat([dfSigcomm, df], ignore_index=True)\n",
        "# Print the final DataFrame\n",
        "print(dfSigcomm.head(10))\n",
        "print()\n",
        "print(len(dfSigcomm))"
      ],
      "metadata": {
        "id": "HpmYcWthZOPW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9177ef8f-dc42-4288-a79e-65613b9587eb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                        name  year   committee  \\\n",
            "0            Pablo Rodriguez  2009  Organizing   \n",
            "1             Ernst Biersack  2009  Organizing   \n",
            "2  Konstantina  Papagiannaki  2009  Organizing   \n",
            "3                Luigi Rizzo  2009  Organizing   \n",
            "4            Christophe Diot  2009  Organizing   \n",
            "5                Dolors Sala  2009  Organizing   \n",
            "6      Jaudelice de Oliveira  2009  Organizing   \n",
            "7  Balachander Krishnamurthy  2009  Organizing   \n",
            "8               Ant Rowstron  2009  Organizing   \n",
            "9              Laurent Mathy  2009  Organizing   \n",
            "\n",
            "                            region  \n",
            "0       Telefonica Research, Spain  \n",
            "1                  Eurecom, France  \n",
            "2       Intel Labs Pittsburgh, USA  \n",
            "3        Universit� di Pisa, Italy  \n",
            "4                  Thomson, France  \n",
            "5  Universitat Pompeu Fabra, Spain  \n",
            "6           Drexel University, USA  \n",
            "7          AT&T Labs-Research, USA  \n",
            "8           Microsoft Research, UK  \n",
            "9         Lancaster University, UK  \n",
            "\n",
            "428\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop through the years 2009-2023\n",
        "for year in range(2009, 2024):\n",
        "  if year >= 2012:\n",
        "    # Determine the URL based on the year\n",
        "    if year in [2016, 2015, 2014]:\n",
        "      url = f\"https://conferences.sigcomm.org/sigcomm/{year}/pc.php\"\n",
        "    elif year in [2013, 2012]:\n",
        "      url = f\"https://conferences.sigcomm.org/sigcomm/{year}/pclist.php\"\n",
        "    else:\n",
        "      url = f\"https://conferences.sigcomm.org/sigcomm/{year}/tpc.html\"\n",
        "    # Send a GET request to the URL and retrieve the response\n",
        "    response = requests.get(url)\n",
        "    # Create a BeautifulSoup object to parse the HTML response\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "    # Select the HTML elements that contain the names of committee members\n",
        "    name_elements = soup.select('div.ui-grid-a div.ui-block-a p')\n",
        "    # Extract the names from the HTML elements and strip any whitespace\n",
        "    member_name = [name.text.strip() for name in name_elements]\n",
        "    # Select the HTML elements that contain the regions of committee members\n",
        "    region_elements = soup.select('div.ui-grid-a div.ui-block-b p')\n",
        "    # Extract the regions from the HTML elements and strip any whitespace\n",
        "    region_name = [region.text.strip() for region in region_elements]\n",
        "    #add data to a dataframe and concatenate with the original\n",
        "    df = pd.DataFrame({'name': member_name, 'year': year, 'committee': 'Program', 'region': region_name})\n",
        "    dfSigcomm = pd.concat([dfSigcomm, df], ignore_index=False)\n",
        "  else:\n",
        "    # Create the URL with the specified year\n",
        "    url = f\"https://conferences.sigcomm.org/sigcomm/{year}/organization.php\"\n",
        "    # Send a GET request to the URL and retrieve the response\n",
        "    response = requests.get(url)\n",
        "    # Create a BeautifulSoup object to parse the HTML response\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "    # Find the <div> element with the id \"contents\"\n",
        "    contents_div = soup.find(\"div\", id=\"contents\")\n",
        "    if contents_div:\n",
        "        if year == 2010:\n",
        "            # Find the <h2> element with the text \"Organizing Committee\"\n",
        "            committee_header = contents_div.find(\"h2\", string=\"Technical Program Committee\")\n",
        "        else:\n",
        "            # Find the <h2> element with the text \"Committee\"\n",
        "            committee_header = contents_div.find(\"h2\", string=\"Program Committee\")\n",
        "        if committee_header:\n",
        "            # Find the table following the <h2> element\n",
        "            committee_table = committee_header.find_next_sibling(\"table\")\n",
        "            if committee_table:\n",
        "                # Find all <tr> elements within the table's <tbody>\n",
        "                committee_members = committee_table.find_all(\"tr\")\n",
        "                for member in committee_members:\n",
        "                      # Find the first <td> element within each <tr> which will have the name\n",
        "                      name_cell = member.find_all(\"td\")\n",
        "                      if name_cell:\n",
        "                          #search for the other <td> element which will have the region\n",
        "                          member_name = name_cell[0].get_text(strip=True)\n",
        "                          if len(name_cell) >= 2:\n",
        "                              region = name_cell[1]\n",
        "                              region_name = region.get_text(strip=True)\n",
        "                              # Add the name to the final DataFrame via concatenation\n",
        "                              df = pd.DataFrame({'name': [member_name], 'year': year, 'committee': 'Program', 'region': [region_name]})\n",
        "                              dfSigcomm = pd.concat([dfSigcomm, df], ignore_index=True)\n",
        "                          else:\n",
        "                              df = pd.DataFrame({'name': [member_name], 'year': year, 'committee': 'Program'})\n",
        "                              dfSigcomm = pd.concat([dfSigcomm, df], ignore_index=True)\n",
        "\n",
        "# Print the final DataFrame\n",
        "print(dfSigcomm.head(5))\n",
        "print(len(dfSigcomm))\n",
        "print(dfSigcomm.groupby(['year', 'committee'])['year', 'committee'].count())"
      ],
      "metadata": {
        "id": "N25cUHBJZZG5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55920f13-ee7f-45e6-bb26-a8eb0c00d1f4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                        name  year   committee                      region\n",
            "0            Pablo Rodriguez  2009  Organizing  Telefonica Research, Spain\n",
            "1             Ernst Biersack  2009  Organizing             Eurecom, France\n",
            "2  Konstantina  Papagiannaki  2009  Organizing  Intel Labs Pittsburgh, USA\n",
            "3                Luigi Rizzo  2009  Organizing   Universit� di Pisa, Italy\n",
            "4            Christophe Diot  2009  Organizing             Thomson, France\n",
            "1251\n",
            "                 year  committee\n",
            "year committee                  \n",
            "2009 Organizing    21         21\n",
            "     Program       60         60\n",
            "2010 Organizing    24         24\n",
            "     Program       50         50\n",
            "2011 Organizing    17         17\n",
            "     Program       52         52\n",
            "2012 Organizing    18         18\n",
            "     Program       51         51\n",
            "2013 Organizing    25         25\n",
            "     Program       35         35\n",
            "2014 Organizing    26         26\n",
            "     Program       54         54\n",
            "2015 Organizing    33         33\n",
            "     Program       51         51\n",
            "2016 Organizing    33         33\n",
            "     Program       56         56\n",
            "2017 Organizing    33         33\n",
            "     Program       59         59\n",
            "2018 Organizing    38         38\n",
            "     Program       50         50\n",
            "2019 Organizing    44         44\n",
            "     Program       61         61\n",
            "2020 Organizing    26         26\n",
            "     Program       56         56\n",
            "2021 Organizing    31         31\n",
            "     Program       60         60\n",
            "2022 Organizing    33         33\n",
            "     Program       67         67\n",
            "2023 Organizing    26         26\n",
            "     Program       61         61\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-9d06825e4c55>:67: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
            "  print(dfSigcomm.groupby(['year', 'committee'])['year', 'committee'].count())\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Making adjustments to the dataframe to proceed with further analysis."
      ],
      "metadata": {
        "id": "KmQlxiQXJr19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#removing any duplicate names that appear in the same year and committee type\n",
        "grouped = dfSigcomm.groupby(['year', 'committee'])\n",
        "dfSigcomm = dfSigcomm[~grouped['name'].transform(lambda x: x.duplicated())]\n",
        "#reindexing the dataframe to avoid future problems\n",
        "dfSigcomm.reset_index(drop=True, inplace=True)\n",
        "#printing\n",
        "print(dfSigcomm.head(5))\n",
        "print(len(dfSigcomm))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKbj3BZ3Zd0T",
        "outputId": "a5eca95c-9073-485c-c22c-47d19ee8e107"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                        name  year   committee                      region\n",
            "0            Pablo Rodriguez  2009  Organizing  Telefonica Research, Spain\n",
            "1             Ernst Biersack  2009  Organizing             Eurecom, France\n",
            "2  Konstantina  Papagiannaki  2009  Organizing  Intel Labs Pittsburgh, USA\n",
            "3                Luigi Rizzo  2009  Organizing   Universit� di Pisa, Italy\n",
            "4            Christophe Diot  2009  Organizing             Thomson, France\n",
            "1242\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Handling the case where names where unprintable characters are displayed for names or regions. We replace these with the actual characters using Unidecode and dictionary mappings."
      ],
      "metadata": {
        "id": "65bgLn8CZ7tp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to decode the string with fallback encoding\n",
        "def decode_with_fallback(text, primary_encoding, fallback_encoding):\n",
        "    try:\n",
        "        if isinstance(text, str):\n",
        "            return text.encode(primary_encoding).decode(primary_encoding)\n",
        "        else:\n",
        "            return str(text).encode(fallback_encoding).decode(fallback_encoding)\n",
        "    except UnicodeDecodeError:\n",
        "        return str(text).encode(fallback_encoding).decode(fallback_encoding)\n",
        "\n",
        "# Assign existing dataframe to a new one\n",
        "dfOutput = dfSigcomm\n",
        "# Apply the decoding function to create the 'cleanedName' and 'cleanedRegion' columns\n",
        "dfOutput['cleanedName'] = dfOutput['name'].apply(lambda x: decode_with_fallback(x, 'utf-8', 'unicode_escape'))\n",
        "dfOutput['cleanedRegion'] = dfOutput['region'].apply(lambda x: decode_with_fallback(x, 'utf-8', 'unicode_escape'))\n",
        "dfOutput['cleanedName'] = dfOutput['cleanedName'].str.replace('�', '')\n",
        "dfOutput['cleanedRegion'] = dfOutput['cleanedRegion'].str.replace('�', '')\n",
        "# Print the updated DataFrame\n",
        "print(dfOutput.head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJeQzPfwmJWD",
        "outputId": "9d7ac5b4-832a-4089-9cb0-c7fa094bb7eb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                        name  year   committee  \\\n",
            "0            Pablo Rodriguez  2009  Organizing   \n",
            "1             Ernst Biersack  2009  Organizing   \n",
            "2  Konstantina  Papagiannaki  2009  Organizing   \n",
            "3                Luigi Rizzo  2009  Organizing   \n",
            "4            Christophe Diot  2009  Organizing   \n",
            "5                Dolors Sala  2009  Organizing   \n",
            "6      Jaudelice de Oliveira  2009  Organizing   \n",
            "7  Balachander Krishnamurthy  2009  Organizing   \n",
            "8               Ant Rowstron  2009  Organizing   \n",
            "9              Laurent Mathy  2009  Organizing   \n",
            "\n",
            "                            region                cleanedName  \\\n",
            "0       Telefonica Research, Spain            Pablo Rodriguez   \n",
            "1                  Eurecom, France             Ernst Biersack   \n",
            "2       Intel Labs Pittsburgh, USA  Konstantina  Papagiannaki   \n",
            "3        Universit� di Pisa, Italy                Luigi Rizzo   \n",
            "4                  Thomson, France            Christophe Diot   \n",
            "5  Universitat Pompeu Fabra, Spain                Dolors Sala   \n",
            "6           Drexel University, USA      Jaudelice de Oliveira   \n",
            "7          AT&T Labs-Research, USA  Balachander Krishnamurthy   \n",
            "8           Microsoft Research, UK               Ant Rowstron   \n",
            "9         Lancaster University, UK              Laurent Mathy   \n",
            "\n",
            "                     cleanedRegion  \n",
            "0       Telefonica Research, Spain  \n",
            "1                  Eurecom, France  \n",
            "2       Intel Labs Pittsburgh, USA  \n",
            "3         Universit di Pisa, Italy  \n",
            "4                  Thomson, France  \n",
            "5  Universitat Pompeu Fabra, Spain  \n",
            "6           Drexel University, USA  \n",
            "7          AT&T Labs-Research, USA  \n",
            "8           Microsoft Research, UK  \n",
            "9         Lancaster University, UK  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-3e5ca6d39358>:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  dfOutput['cleanedName'] = dfOutput['name'].apply(lambda x: decode_with_fallback(x, 'utf-8', 'unicode_escape'))\n",
            "<ipython-input-8-3e5ca6d39358>:15: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  dfOutput['cleanedRegion'] = dfOutput['region'].apply(lambda x: decode_with_fallback(x, 'utf-8', 'unicode_escape'))\n",
            "<ipython-input-8-3e5ca6d39358>:16: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  dfOutput['cleanedName'] = dfOutput['cleanedName'].str.replace('�', '')\n",
            "<ipython-input-8-3e5ca6d39358>:17: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  dfOutput['cleanedRegion'] = dfOutput['cleanedRegion'].str.replace('�', '')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nameDict = {'Ãtalo Cunha':'Italo Cunha',\n",
        "'AndrÃƒÂ¡s CsÃƒÂ¡szÃƒÂ¡r':'Andras Csaszar',\n",
        "'AndrÃ¡s CsÃ¡szÃ¡r':'Andras Csaszar',\n",
        "'BalÃƒÂ¡zs Sonkoly':'Balazs Sonkoly',\n",
        "'BalÃ¡zs Sonkoly':'Balazs Sonkoly',\n",
        "'Emin GÃ¼n Sirer':'Emin Gun Sirer',\n",
        "'FabÃ­ola Greve':'Fabiola Greve',\n",
        "'FabiÃ¡n Bustamante':'Fabian Bustamente',\n",
        "'FabiÃ¡n E. Bustamante':'Fabian E. Bustamente',\n",
        "'FabiÃƒÂ¡n Bustamante':'Fabian Bustamente',\n",
        "'GÃƒÂ¡bor RÃƒÂ©tvÃƒÂ¡ri':'Gabor Retvari',\n",
        "'GÃ¡bor RÃ©tvÃ¡ri': 'Gabor Retvari',\n",
        "'IstvÃƒÂ¡n GÃƒÂ³dor':'Istvan Godor',\n",
        "'IstvÃ¡n GÃ³dor':'Istvan Godor',\n",
        "'JÃ¶rg Liebeherr':'Jaorg Liebeherr',\n",
        "'JÃ¶rg Ott':'Jaorg Ott',\n",
        "'JÃƒÂ¡nos Tapolcai':'Janos Tapolcai',\n",
        "'JÃ¡nos Tapolcai':'Janos Tapolcai',\n",
        "'Matthias WÃƒÂ¤hlisch':'Matthias Waschlisch',\n",
        "'Matthias WÃ¤hlisch':'Matthias Waschlisch',\n",
        "'Robert SoulÃƒÂ©':'Robert Soulac',\n",
        "'Robert SoulÃ©': 'Robert Soulac',\n",
        "'RubÃƒÂ©n Cuevas':'Rubaon Cuevas',\n",
        "'RubÃ©n Cuevas':'Rubaon Cuevas',\n",
        "'ZalÃƒÂ¡n Heszberger':'Zalan Heszberger',\n",
        "'ZalÃ¡n Heszberger':'Zalan Heszberger',\n",
        "'ZoltÃƒÂ¡n LÃƒÂ¡zÃƒÂ¡r':'Zoltan Lazar',\n",
        "'ZoltÃ¡n LÃ¡zÃ¡r':'Zoltan Lazar',\n",
        "'Cecilia Testart/p>': 'Cecilia Testart'}\n",
        "dfOutput['cleanedName'] = dfOutput['cleanedName'].map(nameDict).fillna(dfOutput['cleanedName'])\n",
        "print(dfOutput['cleanedName'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2xdfozpMdrS",
        "outputId": "6e43f7b0-a812-493c-b4a8-18c8233250c0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0                 Pablo Rodriguez\n",
            "1                  Ernst Biersack\n",
            "2       Konstantina  Papagiannaki\n",
            "3                     Luigi Rizzo\n",
            "4                 Christophe Diot\n",
            "                  ...            \n",
            "1237                   Yiting Xia\n",
            "1238                 Yiying Zhang\n",
            "1239                       Yu Hua\n",
            "1240                   Yunxin Liu\n",
            "1241                Zhizhen Zhong\n",
            "Name: cleanedName, Length: 1242, dtype: object\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-326921f1e225>:30: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  dfOutput['cleanedName'] = dfOutput['cleanedName'].map(nameDict).fillna(dfOutput['cleanedName'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "regionDict = {'Universitï¿½ di Pisa, Italy':'Universita di Pisa',\n",
        "'Universitat Politï¿½cnica de Catalunya, Spain':'Universitat Politecnica de Catalunya',\n",
        "'Universidad Autï¿½noma de Madrid, Spain':'Universidad Autonoma de Madrid',\n",
        "'Freie UniversitÃƒÂ¤t Berlin, Germany':'Freie Universitat Berlin',\n",
        "'Freie UniversitÃ¤t Berlin, Germany':'Freie Universitat Berlin',\n",
        "'University of WisconsinÃ¢Â€Â“Madison':'University of Wisconsin-Madison',\n",
        "'University of WisconsinâMadison':'University of Wisconsin-Madison',\n",
        "'Technische UniversitÃƒÂ¤t MÃƒÂ¼nchen':'Technische Universitat Munchen',\n",
        "'Technische UniversitÃ¤t MÃ¼nchen':'Technische Universitat Munchen',\n",
        "'UniversitÃ  di Pisa, Italy, USA':'Universita di Pisa',\n",
        "'ETH ZÃƒÂ¼rich':'ETH Zurich',\n",
        "'ETH ZÃ¼rich':'ETH Zurich',\n",
        "'UniversitÃƒÂ  della Svizzera italiana':'Universita della Svizzera Italiana',\n",
        "'UniversitÃƒÂ© catholique de Louvain':'Universite Catholique de Louvain',\n",
        "'''UniversitÃ© catholique de\n",
        "              Louvain''':'Universite Catholique de Louvain',\n",
        "'University of Illinois UrbanaÃ¢Â€Â“Champaign':'University of Illinois Urbana-Champaign',\n",
        "'University of Illinois UrbanaâChampaign':'University of Illinois Urbana-Champaign',\n",
        "'AT&T Labs - Research, USANikolaos LaoutarisTelefonica Research, SpainRatul MahajanMicrosoft Research, USADavid OranCisco, USAJitendra PadhyeMicrosoft Research, USAVenkat PadmanabhanMicrosoft Research, IndiaAdrian PerrigCarnegie Mellon University, USALili QiuUniversity of Texas at Austin, USAByrav RamamurthyUniversity of Nebraska-Lincoln, USADanny RazTechnion, IsraelScott RixnerRice University, USATimothy RoscoeETH Zurich, SwitzerlandMema RoussopoulosUniversity of Athens, GreeceStefan SavageU.C. San Diego, USASrinivasan SeshanCarnegie Mellon University, USADevavrat ShahMIT, USAScott ShenkerU.C. Berkeley, USAEmin Gün SirerCornell University, USAVijay SivaramanUniversity of New South Wales, AustraliaKun TanMicrosoft Research, ChinaAmin VahdatGoogle / U.C. San Diego, USAHelen WangMicrosoft Research, USAGordon WilfongBell Labs Research, USAWalter WillingerAT&T Labs - Research, USAAlec WolmanMicrosoft Research, USAYinglian XieMicrosoft Research, USARichard YangYale University, USAHaifeng YuNational University of Singapore, SingaporeEllen ZeguraGeorgia Tech, USAYin ZhangUniversity of Texas at Austin, USAYongguang ZhangMicrosoft Research, ChinaHeather ZhengU.C. Santa Barbara, USA':'AT&T Labs'}\n",
        "dfOutput['cleanedRegion'] = dfOutput['cleanedRegion'].map(nameDict).fillna(dfOutput['cleanedRegion'])\n",
        "print(dfOutput['cleanedRegion'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVYhMZbjUY7d",
        "outputId": "6faad5a1-b3d7-4d98-dfdd-0c58feea948e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0                              Telefonica Research, Spain\n",
            "1                                         Eurecom, France\n",
            "2                              Intel Labs Pittsburgh, USA\n",
            "3                                Universit di Pisa, Italy\n",
            "4                                         Thomson, France\n",
            "                              ...                        \n",
            "1237                 Max Planck Institute for Informatics\n",
            "1238                  University of California, San Diego\n",
            "1239        Huazhong University of Science and Technology\n",
            "1240    Institute for AI Industry Research (AIR), Tsin...\n",
            "1241                Massachusetts Institute of Technology\n",
            "Name: cleanedRegion, Length: 1242, dtype: object\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-b9032086f128>:20: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  dfOutput['cleanedRegion'] = dfOutput['cleanedRegion'].map(nameDict).fillna(dfOutput['cleanedRegion'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print column names and data types\n",
        "for column_name, column_type in zip(dfOutput.columns, dfOutput.dtypes):\n",
        "    print(f\"Column Name: {column_name}, Data Type: {column_type}\")\n",
        "print(len(dfOutput))\n",
        "print(len(dfSigcomm))\n",
        "print(dfOutput.head(5))\n",
        "print()\n",
        "print(dfOutput['name'].head(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCVKwz6-qMiX",
        "outputId": "f1f39c0c-f7f6-4429-9efb-03fbcad7df31"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column Name: name, Data Type: object\n",
            "Column Name: year, Data Type: object\n",
            "Column Name: committee, Data Type: object\n",
            "Column Name: region, Data Type: object\n",
            "Column Name: cleanedName, Data Type: object\n",
            "Column Name: cleanedRegion, Data Type: object\n",
            "1242\n",
            "1242\n",
            "                        name  year   committee                      region  \\\n",
            "0            Pablo Rodriguez  2009  Organizing  Telefonica Research, Spain   \n",
            "1             Ernst Biersack  2009  Organizing             Eurecom, France   \n",
            "2  Konstantina  Papagiannaki  2009  Organizing  Intel Labs Pittsburgh, USA   \n",
            "3                Luigi Rizzo  2009  Organizing   Universit� di Pisa, Italy   \n",
            "4            Christophe Diot  2009  Organizing             Thomson, France   \n",
            "\n",
            "                 cleanedName               cleanedRegion  \n",
            "0            Pablo Rodriguez  Telefonica Research, Spain  \n",
            "1             Ernst Biersack             Eurecom, France  \n",
            "2  Konstantina  Papagiannaki  Intel Labs Pittsburgh, USA  \n",
            "3                Luigi Rizzo    Universit di Pisa, Italy  \n",
            "4            Christophe Diot             Thomson, France  \n",
            "\n",
            "0              Pablo Rodriguez\n",
            "1               Ernst Biersack\n",
            "2    Konstantina  Papagiannaki\n",
            "3                  Luigi Rizzo\n",
            "4              Christophe Diot\n",
            "Name: name, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we use the geography libraries: If the institution read from the SIGCOMM website has a country listed, it will add the continent of the country to a dataframe column. If instead the institution is a specific location (like a university) but doesn't have the country listed, we map the location to a country which will be used to determine the continent."
      ],
      "metadata": {
        "id": "s_ZKhScbfDA3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_continent(place):\n",
        "    # Mapping of location abbreviations to country names\n",
        "    location_mappings = {\n",
        "        'NUS': 'Singapore',\n",
        "        'Columbia|US|USA|Harvard|Stanford|George Washington|Irvine|West Point|Connecticut|Princeton|Purdue|Minnesota|Austin|San Diego|Pennsylvania|NYU|Chicago|Virginia|MIT|Georgia|Duke|Northeastern|California|Northwestern|Massachusetts|Carnegie|UCLA|CMU|Stony Brook|Wisconsin|Wisconsin-Madison|Rice|Colorado|Rutgers|Illinois|Boston|Florida|New York|Maryland|Michigan|Yale|Washington|Texas|Carolina|UC|Cornell|Brown|Oregon|USC|UCSB|Amherst|UIUC|Johns Hopkins|UWM|UWisc|Brown|UMich|UMass|Redmond|ICSI': 'United States',\n",
        "        'UK|London|Cambridge|Oxford|Southampton': 'United Kingdom',\n",
        "        'Karlstad|KTH': 'Sweden',\n",
        "        'Hong Kong|Chiao|Huazhong|Tsinghua|Fudan|Peking|HKUST|NTU': 'China',\n",
        "        'EPFL|Zurich|Svizzera|ETHZ': 'Switzerland',\n",
        "        'MPI-SWS|Planck|Munich|Brandenburg|Technische|Potsdam|MPI': 'Germany',\n",
        "        'Delft|Amsterdam|Twente': 'Netherlands',\n",
        "        'Jerusalem|Reichman': 'Israel',\n",
        "        'KAIST|Korea': 'South Korea',\n",
        "        'UNSW|Melbourne|Adelaide': 'Australia',\n",
        "        'Toronto|Waterloo': 'Canada',\n",
        "        'Budapest': 'Hungary',\n",
        "        'Abdullah': 'South Arabia',\n",
        "        'IMDEA': 'Spain',\n",
        "        'Linz|AIT|Austrian': 'Austria',\n",
        "        'Campinas|Passo Fundo': 'Brazil',\n",
        "        'Lisbon': 'Portugal',\n",
        "        'IIT': 'India',\n",
        "        'UCLouvain|Louvain': 'Belgium',\n",
        "        'Lahore|LUMS': 'Pakistan',\n",
        "        'Waikato': 'New Zealand',\n",
        "        'Bucharest': 'Romania',\n",
        "        'Inria': 'France',\n",
        "        'IIJ|NII': 'Japan'\n",
        "    }\n",
        "    # Wording manipulations to extract mappings\n",
        "    for abbreviation, full_name in location_mappings.items():\n",
        "        abbreviation_list = re.split(r'\\||\\s+', abbreviation)\n",
        "        abbreviation_regex = r'\\b({})\\b'.format('|'.join(re.escape(abbr) for abbr in abbreviation_list))\n",
        "        place = re.sub(abbreviation_regex, full_name, place, flags=re.IGNORECASE)\n",
        "\n",
        "    # Extract country from the place name\n",
        "    countries = GeoText(place).countries\n",
        "    if countries:\n",
        "        country = countries[0]\n",
        "    else:\n",
        "        return ''\n",
        "\n",
        "    # Remove extra text after country name\n",
        "    place = place.replace(country, '').strip(', ')\n",
        "\n",
        "    # Get continent based on country\n",
        "    geonames = GeonamesCache()\n",
        "    country_info = geonames.get_countries_by_names().get(country)\n",
        "    if country_info:\n",
        "        continent = country_info['continentcode']\n",
        "        if not continent:\n",
        "            return ''\n",
        "    else:\n",
        "        return ''\n",
        "\n",
        "    # If additional text exists, extract country from it\n",
        "    if place:\n",
        "        additional_countries = GeoText(place).countries\n",
        "        if additional_countries:\n",
        "            additional_country = additional_countries[0]\n",
        "            additional_country_info = geonames.get_countries_by_names().get(additional_country)\n",
        "            if additional_country_info:\n",
        "                continent = additional_country_info['continentcode']\n",
        "                if continent:\n",
        "                    return continent\n",
        "\n",
        "    return continent\n",
        "\n",
        "# Apply the get_continent function to the 'cleanedRegion' column in the DataFrame\n",
        "dfOutput['Continent'] = dfOutput['cleanedRegion'].apply(get_continent)\n",
        "dfOutput['Continent'].replace('NA', 'NorAm', inplace=True)\n",
        "# Print the updated DataFrame\n",
        "print(dfOutput.head(10))"
      ],
      "metadata": {
        "id": "j5F9pHtqo7Tu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "569e6dbb-190f-4721-e925-f6822f8d093e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                        name  year   committee  \\\n",
            "0            Pablo Rodriguez  2009  Organizing   \n",
            "1             Ernst Biersack  2009  Organizing   \n",
            "2  Konstantina  Papagiannaki  2009  Organizing   \n",
            "3                Luigi Rizzo  2009  Organizing   \n",
            "4            Christophe Diot  2009  Organizing   \n",
            "5                Dolors Sala  2009  Organizing   \n",
            "6      Jaudelice de Oliveira  2009  Organizing   \n",
            "7  Balachander Krishnamurthy  2009  Organizing   \n",
            "8               Ant Rowstron  2009  Organizing   \n",
            "9              Laurent Mathy  2009  Organizing   \n",
            "\n",
            "                            region                cleanedName  \\\n",
            "0       Telefonica Research, Spain            Pablo Rodriguez   \n",
            "1                  Eurecom, France             Ernst Biersack   \n",
            "2       Intel Labs Pittsburgh, USA  Konstantina  Papagiannaki   \n",
            "3        Universit� di Pisa, Italy                Luigi Rizzo   \n",
            "4                  Thomson, France            Christophe Diot   \n",
            "5  Universitat Pompeu Fabra, Spain                Dolors Sala   \n",
            "6           Drexel University, USA      Jaudelice de Oliveira   \n",
            "7          AT&T Labs-Research, USA  Balachander Krishnamurthy   \n",
            "8           Microsoft Research, UK               Ant Rowstron   \n",
            "9         Lancaster University, UK              Laurent Mathy   \n",
            "\n",
            "                     cleanedRegion Continent  \n",
            "0       Telefonica Research, Spain        EU  \n",
            "1                  Eurecom, France        EU  \n",
            "2       Intel Labs Pittsburgh, USA     NorAm  \n",
            "3         Universit di Pisa, Italy        EU  \n",
            "4                  Thomson, France        EU  \n",
            "5  Universitat Pompeu Fabra, Spain        EU  \n",
            "6           Drexel University, USA     NorAm  \n",
            "7          AT&T Labs-Research, USA     NorAm  \n",
            "8           Microsoft Research, UK        EU  \n",
            "9         Lancaster University, UK        EU  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-93c951e91489>:70: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  dfOutput['Continent'] = dfOutput['cleanedRegion'].apply(get_continent)\n",
            "<ipython-input-12-93c951e91489>:71: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  dfOutput['Continent'].replace('NA', 'NorAm', inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filling in values which are still missing or need to be changed."
      ],
      "metadata": {
        "id": "8Q_NQFj4fzaG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dict = {\n",
        "    \"David Oran\": (\"Unknown\", \"NorAm\"),\n",
        "    \"Sam Leffler\": (\"Google\", \"NorAm\"),\n",
        "    \"Balachander Krishnamurthy\": (\"AT&T Labs\", \"NorAm\")\n",
        "}\n",
        "for name, (region, continent) in data_dict.items():\n",
        "    dfOutput.loc[dfOutput['cleanedName'] == name, 'region'] = region\n",
        "    dfOutput.loc[dfOutput['cleanedName'] == name, 'cleanedRegion'] = region\n",
        "    dfOutput.loc[dfOutput['cleanedName'] == name, 'Continent'] = continent"
      ],
      "metadata": {
        "id": "cOU-jQ9TAKe-"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "updates = [\n",
        "    (8, \"Antony Rowstron\"),\n",
        "    (25, \"Geoff Voelker\"),\n",
        "    (47, \"Jeff Mogul\"),\n",
        "    (50, \"Kenneth L. Calvert\"),\n",
        "    (62, \"Jaorg Ott\"),\n",
        "    (324, \"Tianbai Ma\"),\n",
        "    (340, 'Ramakrishnan Durairajan'),\n",
        "    (341, 'David Choffnes'),\n",
        "    (444, 'Samuel J. Leffler'),\n",
        "    (460, 'Jennifer Rexford'),\n",
        "    (480, 'Geoff Voelker'),\n",
        "    (509, 'Ramachandran Ramjee'),\n",
        "    (530, 'Jeff Mogul'),\n",
        "    (539, 'Mike Freedman'),\n",
        "    (588, 'Matthew Caesar 0001'),\n",
        "    (612, 'David Maltz'),\n",
        "    (615, 'Michael Mitzenmacher'),\n",
        "    (621, 'Antony Rowstron'),\n",
        "    (626, 'Lakshminarayanan Subramanian'),\n",
        "    (639, 'Matthew Caesar 0001'),\n",
        "    (645, 'Timothy G. Griffin'),\n",
        "    (652, 'Dave Levin'),\n",
        "    (674, 'Matthew Caesar 0001'),\n",
        "    (880, 'David Andersen'),\n",
        "    (1073, 'Jennifer Rexford'),\n",
        "    (1084, 'Matthew Caesar 0001'),\n",
        "    (1197, 'Elizabeth Belding'),\n",
        "    (1198, 'Fabian E. Bustamante')\n",
        "]\n",
        "for index, name in updates:\n",
        "  dfOutput.at[index, 'name'] = name\n",
        "  dfOutput.at[index, 'cleanedName'] = name"
      ],
      "metadata": {
        "id": "j3Kiw177Vqa5"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# removing any members considered as \"Additional/External Reviewers\"\n",
        "indices_to_remove = [880, 881, 882, 883, 884, 885, 886, 1176, 1177, 1178, 1179, 1180]\n",
        "dfOutput = dfOutput.drop(indices_to_remove)\n",
        "dfOutput = dfOutput.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "rC12ID4T_TMG"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Grouping the dataframe by year and committee\n",
        "grouped = dfOutput.groupby(['year', 'committee'])\n",
        "\n",
        "# Checking for duplicate names within each group\n",
        "duplicates = grouped['name'].apply(lambda x: x[x.duplicated()])\n",
        "\n",
        "if duplicates.empty:\n",
        "    print(\"No duplicates exist.\")\n",
        "else:\n",
        "    print(\"Duplicate names found:\")\n",
        "    for duplicate in duplicates:\n",
        "        print(duplicate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XS-YSUTuFbVo",
        "outputId": "bc4fbbb7-0964-4ce3-c788-42ba70a68cdd"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No duplicates exist.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We write the current dataframe into a CSV file which will be read and analyzed in the next Google Colab."
      ],
      "metadata": {
        "id": "IbArvCaJgA7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfOutput.to_csv(\"/content/gdrive/My Drive/Colab Notebooks/save_data/people_weave/SIGCOMMNames.csv\", sep=',', index=False, encoding='utf-8')"
      ],
      "metadata": {
        "id": "pTPZK6VotgtZ"
      },
      "execution_count": 18,
      "outputs": []
    }
  ]
}